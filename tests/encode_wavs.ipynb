{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import EncodecModel, AutoProcessor\n",
    "import torchaudio\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "import pinecone\n",
    "from utils.encoder_utils import encode_pca_file_folder\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "pinecone_key = os.getenv(\"PINECONE_KEY\")\n",
    "pinecone_environment = os.getenv(\"PINECONE_ENV\")\n",
    "\n",
    "pinecone.init(api_key=pinecone_key, environment=pinecone_environment)\n",
    "\n",
    "model = EncodecModel.from_pretrained(\"facebook/encodec_48khz\")\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/encodec_48khz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_samples(audio_sample): # load the model + processor (for pre-processing the audio)\n",
    "    model = EncodecModel.from_pretrained(\"facebook/encodec_48khz\")\n",
    "    processor = AutoProcessor.from_pretrained(\"facebook/encodec_48khz\")\n",
    "\n",
    "    # pre-process the inputs\n",
    "    inputs = processor(raw_audio=audio_sample, sampling_rate=processor.sampling_rate, return_tensors=\"pt\")\n",
    "\n",
    "    # explicitly encode then decode the audio inputs\n",
    "    #encoder_outputs = model.encode(inputs[\"input_values\"], inputs[\"padding_mask\"])\n",
    "    #audio_values = model.decode(encoder_outputs.audio_codes, encoder_outputs.audio_scales, inputs[\"padding_mask\"])[0]\n",
    "\n",
    "    # or the equivalent with a forward pass\n",
    "    audio_values = model(inputs[\"input_values\"], inputs[\"padding_mask\"]).audio_values\n",
    "\n",
    "    # you can also extract the discrete codebook representation for LM tasks\n",
    "    # output: concatenated tensor of all the representations\n",
    "    audio_codes = model(inputs[\"input_values\"], inputs[\"padding_mask\"]).audio_codes\n",
    "\n",
    "    return audio_values, audio_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39m# Encode the segment\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39m# Encode using the model\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 29\u001b[0m     codes \u001b[39m=\u001b[39m encode_samples(segment)\n\u001b[0;32m     31\u001b[0m \u001b[39m# Update metadata with encoded audio and audio_codes\u001b[39;00m\n\u001b[0;32m     32\u001b[0m metadata[\u001b[39m\"\u001b[39m\u001b[39maudio_codes\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m codes\u001b[39m.\u001b[39mnumpy()  \u001b[39m# Convert to numpy for easier storage\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 13\u001b[0m, in \u001b[0;36mencode_samples\u001b[1;34m(audio_sample)\u001b[0m\n\u001b[0;32m     10\u001b[0m audio_values \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mdecode(encoder_outputs\u001b[39m.\u001b[39maudio_codes, encoder_outputs\u001b[39m.\u001b[39maudio_scales, inputs[\u001b[39m\"\u001b[39m\u001b[39mpadding_mask\u001b[39m\u001b[39m\"\u001b[39m])[\u001b[39m0\u001b[39m]\n\u001b[0;32m     12\u001b[0m \u001b[39m# or the equivalent with a forward pass\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m audio_values \u001b[39m=\u001b[39m model(inputs[\u001b[39m\"\u001b[39;49m\u001b[39minput_values\u001b[39;49m\u001b[39m\"\u001b[39;49m], inputs[\u001b[39m\"\u001b[39;49m\u001b[39mpadding_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39maudio_values\n\u001b[0;32m     15\u001b[0m \u001b[39m# you can also extract the discrete codebook representation for LM tasks\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m# output: concatenated tensor of all the representations\u001b[39;00m\n\u001b[0;32m     17\u001b[0m audio_codes \u001b[39m=\u001b[39m model(inputs[\u001b[39m\"\u001b[39m\u001b[39minput_values\u001b[39m\u001b[39m\"\u001b[39m], inputs[\u001b[39m\"\u001b[39m\u001b[39mpadding_mask\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39maudio_codes\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:805\u001b[0m, in \u001b[0;36mEncodecModel.forward\u001b[1;34m(self, input_values, padding_mask, bandwidth, audio_codes, audio_scales, return_dict)\u001b[0m\n\u001b[0;32m    802\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou specified `audio_scales` but did not specify the `audio_codes`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    804\u001b[0m \u001b[39mif\u001b[39;00m audio_scales \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m audio_codes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 805\u001b[0m     audio_codes, audio_scales \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode(input_values, padding_mask, bandwidth, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    807\u001b[0m audio_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(audio_codes, audio_scales, padding_mask, return_dict\u001b[39m=\u001b[39mreturn_dict)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    808\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:646\u001b[0m, in \u001b[0;36mEncodecModel.encode\u001b[1;34m(self, input_values, padding_mask, bandwidth, return_dict)\u001b[0m\n\u001b[0;32m    644\u001b[0m mask \u001b[39m=\u001b[39m padding_mask[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, offset : offset \u001b[39m+\u001b[39m chunk_length]\u001b[39m.\u001b[39mbool()\n\u001b[0;32m    645\u001b[0m frame \u001b[39m=\u001b[39m input_values[:, :, offset : offset \u001b[39m+\u001b[39m chunk_length]\n\u001b[1;32m--> 646\u001b[0m encoded_frame, scale \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_frame(frame, bandwidth, mask)\n\u001b[0;32m    647\u001b[0m encoded_frames\u001b[39m.\u001b[39mappend(encoded_frame)\n\u001b[0;32m    648\u001b[0m scales\u001b[39m.\u001b[39mappend(scale)\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:580\u001b[0m, in \u001b[0;36mEncodecModel._encode_frame\u001b[1;34m(self, input_values, bandwidth, padding_mask)\u001b[0m\n\u001b[0;32m    577\u001b[0m     input_values \u001b[39m=\u001b[39m input_values \u001b[39m/\u001b[39m scale\n\u001b[0;32m    579\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(input_values)\n\u001b[1;32m--> 580\u001b[0m codes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquantizer\u001b[39m.\u001b[39;49mencode(embeddings, bandwidth)\n\u001b[0;32m    581\u001b[0m codes \u001b[39m=\u001b[39m codes\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m    582\u001b[0m \u001b[39mreturn\u001b[39;00m codes, scale\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:423\u001b[0m, in \u001b[0;36mEncodecResidualVectorQuantizer.encode\u001b[1;34m(self, embeddings, bandwidth)\u001b[0m\n\u001b[0;32m    421\u001b[0m all_indices \u001b[39m=\u001b[39m []\n\u001b[0;32m    422\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[:num_quantizers]:\n\u001b[1;32m--> 423\u001b[0m     indices \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mencode(residual)\n\u001b[0;32m    424\u001b[0m     quantized \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mdecode(indices)\n\u001b[0;32m    425\u001b[0m     residual \u001b[39m=\u001b[39m residual \u001b[39m-\u001b[39m quantized\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:387\u001b[0m, in \u001b[0;36mEncodecVectorQuantization.encode\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[0;32m    386\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m--> 387\u001b[0m     embed_in \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcodebook\u001b[39m.\u001b[39;49mencode(hidden_states)\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m embed_in\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:366\u001b[0m, in \u001b[0;36mEncodecEuclideanCodebook.encode\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    364\u001b[0m hidden_states \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, shape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]))\n\u001b[0;32m    365\u001b[0m \u001b[39m# quantize\u001b[39;00m\n\u001b[1;32m--> 366\u001b[0m embed_ind \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquantize(hidden_states)\n\u001b[0;32m    367\u001b[0m \u001b[39m# post-process\u001b[39;00m\n\u001b[0;32m    368\u001b[0m embed_ind \u001b[39m=\u001b[39m embed_ind\u001b[39m.\u001b[39mview(\u001b[39m*\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:357\u001b[0m, in \u001b[0;36mEncodecEuclideanCodebook.quantize\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    355\u001b[0m embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed\u001b[39m.\u001b[39mt()\n\u001b[0;32m    356\u001b[0m scaled_states \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 357\u001b[0m dist \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m(scaled_states \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m hidden_states \u001b[39m@\u001b[39m embed \u001b[39m+\u001b[39m embed\u001b[39m.\u001b[39;49mpow(\u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39msum(\u001b[39m0\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[0;32m    358\u001b[0m embed_ind \u001b[39m=\u001b[39m dist\u001b[39m.\u001b[39mmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mindices\n\u001b[0;32m    359\u001b[0m \u001b[39mreturn\u001b[39;00m embed_ind\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "for metadata in metadata_list:\n",
    "    # Load audio file\n",
    "    wav, sr = torchaudio.load(metadata[\"file\"])\n",
    "    \n",
    "    # Calculate segment length and frames per segment\n",
    "    segment_length = 15  # 5 seconds, for example\n",
    "    frames_per_segment = segment_length * sr\n",
    "    \n",
    "    # Split and encode\n",
    "    num_segments = int(len(wav[0]) / frames_per_segment)\n",
    "    for i in range(num_segments):\n",
    "        # Update dynamic metadata fields\n",
    "        metadata[\"segment_id\"] = f\"{metadata['Song Title']}_segment_{i+1}\"\n",
    "        metadata[\"start_time\"] = i * segment_length\n",
    "        metadata[\"end_time\"] = (i + 1) * segment_length\n",
    "        \n",
    "        # Extract segment\n",
    "        segment = wav[:, i * frames_per_segment:(i + 1) * frames_per_segment]\n",
    "\n",
    "        # Serialize the audio segment\n",
    "        audio_bytes = segment.numpy().tobytes()\n",
    "        encoded_audio_str = base64.b64encode(audio_bytes).decode('utf-8')\n",
    "        \n",
    "        # Encode the segment\n",
    "        # Encode using the model\n",
    "        with torch.no_grad():\n",
    "            encoded_frames = model.encode(torch.tensor(processed_audio))\n",
    "        \n",
    "        # Update metadata with encoded audio and audio_codes\n",
    "        metadata[\"audio_codes\"] = codes.numpy()  # Convert to numpy for easier storage\n",
    "        metadata[\"encoded_audio\"] = encoded_audio_str\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor feature size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 1/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 2/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 3/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 4/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 5/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 6/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 7/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 8/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 9/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 10/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 11/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 12/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 13/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 14/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 15/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 265472)\n",
      "Failed to process lc_aint_far_from_it.mp3: list assignment index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "INFO:root:Processing chunk 1/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor feature size: 2\n",
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 2/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 3/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 4/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 5/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 6/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 7/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 8/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 9/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 10/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 11/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 12/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 13/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 14/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 15/14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 216320)\n",
      "Failed to process lc_beautiful_crazy.wav: list assignment index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "INFO:root:Processing chunk 1/17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor feature size: 2\n",
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing chunk 2/17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape after transposing: (2, 720000)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Create a dataframe from encoding the audio files\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# in the \"./audio_files/artist_main/\" folder\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df \u001b[39m=\u001b[39m encode_pca_file_folder(\u001b[39m\"\u001b[39;49m\u001b[39m./audio_files/artist_main/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\OneDrive\\Desktop\\Current Projects\\vl_demo\\tests\\utils\\encoder_utils.py:74\u001b[0m, in \u001b[0;36mencode_pca_file_folder\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m     71\u001b[0m full_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(folder_path, audio_file)\n\u001b[0;32m     72\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     \u001b[39m# Extract and encode audio chunks\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m     encoded_chunks \u001b[39m=\u001b[39m chunk_and_encode_encodec(full_path)\n\u001b[0;32m     76\u001b[0m     \u001b[39m# Apply PCA to each chunk\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     reduced_chunks \u001b[39m=\u001b[39m [pca\u001b[39m.\u001b[39mtransform(chunk) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m encoded_chunks]\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\OneDrive\\Desktop\\Current Projects\\vl_demo\\tests\\utils\\encoder_utils.py:57\u001b[0m, in \u001b[0;36mchunk_and_encode_encodec\u001b[1;34m(audio_file_path, model_name, processor_name)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39m# Your existing code for processing the chunk\u001b[39;00m\n\u001b[0;32m     55\u001b[0m inputs \u001b[39m=\u001b[39m processor(raw_audio\u001b[39m=\u001b[39mchunk, sampling_rate\u001b[39m=\u001b[39msr, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 57\u001b[0m encoder_outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode(inputs[\u001b[39m\"\u001b[39;49m\u001b[39minput_values\u001b[39;49m\u001b[39m\"\u001b[39;49m], inputs[\u001b[39m\"\u001b[39;49m\u001b[39mpadding_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m     59\u001b[0m encoded_chunks[i \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m chunk_size] \u001b[39m=\u001b[39m encoder_outputs\u001b[39m.\u001b[39maudio_codes\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:646\u001b[0m, in \u001b[0;36mEncodecModel.encode\u001b[1;34m(self, input_values, padding_mask, bandwidth, return_dict)\u001b[0m\n\u001b[0;32m    644\u001b[0m mask \u001b[39m=\u001b[39m padding_mask[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, offset : offset \u001b[39m+\u001b[39m chunk_length]\u001b[39m.\u001b[39mbool()\n\u001b[0;32m    645\u001b[0m frame \u001b[39m=\u001b[39m input_values[:, :, offset : offset \u001b[39m+\u001b[39m chunk_length]\n\u001b[1;32m--> 646\u001b[0m encoded_frame, scale \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_frame(frame, bandwidth, mask)\n\u001b[0;32m    647\u001b[0m encoded_frames\u001b[39m.\u001b[39mappend(encoded_frame)\n\u001b[0;32m    648\u001b[0m scales\u001b[39m.\u001b[39mappend(scale)\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:579\u001b[0m, in \u001b[0;36mEncodecModel._encode_frame\u001b[1;34m(self, input_values, bandwidth, padding_mask)\u001b[0m\n\u001b[0;32m    576\u001b[0m     scale \u001b[39m=\u001b[39m mono\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39msqrt() \u001b[39m+\u001b[39m \u001b[39m1e-8\u001b[39m\n\u001b[0;32m    577\u001b[0m     input_values \u001b[39m=\u001b[39m input_values \u001b[39m/\u001b[39m scale\n\u001b[1;32m--> 579\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(input_values)\n\u001b[0;32m    580\u001b[0m codes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquantizer\u001b[39m.\u001b[39mencode(embeddings, bandwidth)\n\u001b[0;32m    581\u001b[0m codes \u001b[39m=\u001b[39m codes\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:302\u001b[0m, in \u001b[0;36mEncodecEncoder.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[0;32m    301\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m--> 302\u001b[0m         hidden_states \u001b[39m=\u001b[39m layer(hidden_states)\n\u001b[0;32m    303\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:166\u001b[0m, in \u001b[0;36mEncodecConv1d.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    161\u001b[0m     padding_left \u001b[39m=\u001b[39m padding_total \u001b[39m-\u001b[39m padding_right\n\u001b[0;32m    162\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pad1d(\n\u001b[0;32m    163\u001b[0m         hidden_states, (padding_left, padding_right \u001b[39m+\u001b[39m extra_padding), mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_mode\n\u001b[0;32m    164\u001b[0m     )\n\u001b[1;32m--> 166\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(hidden_states)\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtime_group_norm\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    169\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(hidden_states)\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:309\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    307\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    308\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 309\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    310\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a dataframe from encoding the audio files\n",
    "# in the \"./audio_files/artist_main/\" folder\n",
    "df = encode_pca_file_folder(\"./audio_files/artist_main/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model = EncodecModel.from_pretrained(\"facebook/encodec_24khz\")\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/encodec_24khz\")\n",
    "\n",
    "def load_audio_data(audio_folder, target_sr, segment_length_seconds: int = 15):\n",
    "    target_sr = processor.sampling_rate\n",
    "    metadata = []\n",
    "    all_segments = []\n",
    "\n",
    "    segment_length_samples = segment_length_seconds * target_sr\n",
    "\n",
    "    for filename in os.listdir(audio_folder):\n",
    "        if filename.endswith('.wav'):\n",
    "            file_path = os.path.join(audio_folder, filename)\n",
    "\n",
    "            # Load and resample the audio\n",
    "            audio_data = load_and_resample_audio(file_path)\n",
    "\n",
    "            # Split audio into segments\n",
    "            segments = split_audio_into_segments(audio_data, segment_length_samples, segment_length_samples)\n",
    "\n",
    "            # Append segments and metadata\n",
    "            for i, segment in enumerate(segments[:5]):\n",
    "                all_segments.append(segment)\n",
    "                segment_metadata = {\n",
    "                    'file': file_path,\n",
    "                    'segment_id': i,\n",
    "                    'start_time': i * segment_length_seconds,\n",
    "                    'end_time': (i * segment_length_seconds) + segment_length_seconds\n",
    "                }\n",
    "                metadata.append(segment_metadata)\n",
    "\n",
    "    return np.array(all_segments), pd.DataFrame(metadata)\n",
    "\n",
    "# Run the function using the audio files \"./audio_files/artist_main\" folder\n",
    "audio_folder = \"./audio_files/artist_main\"\n",
    "\n",
    "# Load the audio data\n",
    "audio_data, audio_metadata = load_audio_data(audio_folder, target_sr=24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected stereo audio but example has 215040 channels",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m librispeech_dummy \u001b[39m=\u001b[39m librispeech_dummy\u001b[39m.\u001b[39mcast_column(\u001b[39m\"\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m\"\u001b[39m, Audio(sampling_rate\u001b[39m=\u001b[39mprocessor\u001b[39m.\u001b[39msampling_rate))\n\u001b[0;32m     10\u001b[0m audio_sample \u001b[39m=\u001b[39m librispeech_dummy[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m---> 11\u001b[0m inputs \u001b[39m=\u001b[39m processor(raw_audio\u001b[39m=\u001b[39;49maudio_sample, sampling_rate\u001b[39m=\u001b[39;49mprocessor\u001b[39m.\u001b[39;49msampling_rate, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     13\u001b[0m encoder_outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode(inputs[\u001b[39m\"\u001b[39m\u001b[39minput_values\u001b[39m\u001b[39m\"\u001b[39m], inputs[\u001b[39m\"\u001b[39m\u001b[39mpadding_mask\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     14\u001b[0m audio_values \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mdecode(encoder_outputs\u001b[39m.\u001b[39maudio_codes, encoder_outputs\u001b[39m.\u001b[39maudio_scales, inputs[\u001b[39m\"\u001b[39m\u001b[39mpadding_mask\u001b[39m\u001b[39m\"\u001b[39m])[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\encodec\\feature_extraction_encodec.py:167\u001b[0m, in \u001b[0;36mEncodecFeatureExtractor.__call__\u001b[1;34m(self, raw_audio, padding, truncation, max_length, return_tensors, sampling_rate)\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected mono audio but example has \u001b[39m\u001b[39m{\u001b[39;00mexample\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m channels\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_size \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m example\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m--> 167\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected stereo audio but example has \u001b[39m\u001b[39m{\u001b[39;00mexample\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m channels\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    169\u001b[0m padded_inputs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    170\u001b[0m input_values \u001b[39m=\u001b[39m BatchFeature({\u001b[39m\"\u001b[39m\u001b[39minput_values\u001b[39m\u001b[39m\"\u001b[39m: raw_audio})\n",
      "\u001b[1;31mValueError\u001b[0m: Expected stereo audio but example has 215040 channels"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "from transformers import EncodecModel, AutoProcessor\n",
    "\n",
    "model = EncodecModel.from_pretrained(\"facebook/encodec_48khz\")\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/encodec_48khz\")\n",
    "librispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "librispeech_dummy = librispeech_dummy.cast_column(\"audio\", Audio(sampling_rate=processor.sampling_rate))\n",
    "audio_sample = librispeech_dummy[-1][\"audio\"][\"array\"]\n",
    "inputs = processor(raw_audio=audio_sample, sampling_rate=processor.sampling_rate, return_tensors=\"pt\")\n",
    "\n",
    "encoder_outputs = model.encode(inputs[\"input_values\"], inputs[\"padding_mask\"])\n",
    "audio_values = model.decode(encoder_outputs.audio_codes, encoder_outputs.audio_scales, inputs[\"padding_mask\"])[0]\n",
    "# or the equivalent with a forward pass\n",
    "audio_values = model(inputs[\"input_values\"], inputs[\"padding_mask\"]).audio_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error encoding audio sample #  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error encoding audio sample #  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error encoding audio sample #  17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error encoding audio sample #  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error encoding audio sample #  19\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the audio data and encode each segment\n",
    "i = 0\n",
    "encoded_audio = []\n",
    "audio_codes = []\n",
    "audio_values = []\n",
    "failed_samples = [] \n",
    "for audio_sample in audio_data:\n",
    "    try:\n",
    "        encoder_outputs, audio_code, audio_value = encode_samples(audio_sample)\n",
    "        encoded_audio.append(encoder_outputs)\n",
    "        audio_codes.append(audio_code)\n",
    "        audio_values.append(audio_value)\n",
    "    except:\n",
    "        print(\"Error encoding audio sample # \", i)\n",
    "        failed_samples.append(i)\n",
    "        pass\n",
    "    i += 1\n",
    "\n",
    "# Append the encoded audio and audio codes to the metadata\n",
    "# for all rows except the ones that failed\n",
    "#audio_metadata['encoded_audio'] = encoded_audio\n",
    "#audio_metadata['audio_codes'] = audio_codes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = audio_metadata.iloc[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>segment_id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./audio_files/artist_main\\lc_beautiful_crazy.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./audio_files/artist_main\\lc_beautiful_crazy.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./audio_files/artist_main\\lc_beautiful_crazy.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./audio_files/artist_main\\lc_beautiful_crazy.wav</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./audio_files/artist_main\\lc_beautiful_crazy.wav</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>./audio_files/artist_main\\lc_fastcar.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>./audio_files/artist_main\\lc_fastcar.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>./audio_files/artist_main\\lc_fastcar.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>./audio_files/artist_main\\lc_fastcar.wav</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>./audio_files/artist_main\\lc_fastcar.wav</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>./audio_files/artist_main\\lc_growinup_gettinol...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>./audio_files/artist_main\\lc_growinup_gettinol...</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>./audio_files/artist_main\\lc_growinup_gettinol...</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>./audio_files/artist_main\\lc_growinup_gettinol...</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>./audio_files/artist_main\\lc_growinup_gettinol...</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 file  segment_id  start_time  \\\n",
       "0    ./audio_files/artist_main\\lc_beautiful_crazy.wav           0           0   \n",
       "1    ./audio_files/artist_main\\lc_beautiful_crazy.wav           1          15   \n",
       "2    ./audio_files/artist_main\\lc_beautiful_crazy.wav           2          30   \n",
       "3    ./audio_files/artist_main\\lc_beautiful_crazy.wav           3          45   \n",
       "4    ./audio_files/artist_main\\lc_beautiful_crazy.wav           4          60   \n",
       "5            ./audio_files/artist_main\\lc_fastcar.wav           0           0   \n",
       "6            ./audio_files/artist_main\\lc_fastcar.wav           1          15   \n",
       "7            ./audio_files/artist_main\\lc_fastcar.wav           2          30   \n",
       "8            ./audio_files/artist_main\\lc_fastcar.wav           3          45   \n",
       "9            ./audio_files/artist_main\\lc_fastcar.wav           4          60   \n",
       "10  ./audio_files/artist_main\\lc_growinup_gettinol...           0           0   \n",
       "11  ./audio_files/artist_main\\lc_growinup_gettinol...           1          15   \n",
       "12  ./audio_files/artist_main\\lc_growinup_gettinol...           2          30   \n",
       "13  ./audio_files/artist_main\\lc_growinup_gettinol...           3          45   \n",
       "14  ./audio_files/artist_main\\lc_growinup_gettinol...           4          60   \n",
       "\n",
       "    end_time  \n",
       "0         15  \n",
       "1         30  \n",
       "2         45  \n",
       "3         60  \n",
       "4         75  \n",
       "5         15  \n",
       "6         30  \n",
       "7         45  \n",
       "8         60  \n",
       "9         75  \n",
       "10        15  \n",
       "11        30  \n",
       "12        45  \n",
       "13        60  \n",
       "14        75  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.7884e-04, -2.5038e-04,  4.6636e-05,  ..., -4.8360e-03,\n",
       "          -5.7032e-03, -5.0412e-03]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mencoded_audio\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m encoded_audio\n\u001b[0;32m      3\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39maudio_codes\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m audio_codes\n\u001b[1;32m----> 4\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39maudio_values\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m audio_values\n\u001b[0;32m      5\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mSong Title\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\py310\\lib\\site-packages\\pandas\\core\\frame.py:3950\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3947\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   3948\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3949\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[1;32m-> 3950\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\py310\\lib\\site-packages\\pandas\\core\\frame.py:4143\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   4134\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4135\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4136\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4141\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4142\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4143\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[0;32m   4145\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   4146\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[0;32m   4147\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   4148\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[0;32m   4149\u001b[0m     ):\n\u001b[0;32m   4150\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4151\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\py310\\lib\\site-packages\\pandas\\core\\frame.py:4871\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4869\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m   4870\u001b[0m     com\u001b[39m.\u001b[39mrequire_length_match(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[1;32m-> 4871\u001b[0m \u001b[39mreturn\u001b[39;00m sanitize_array(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex, copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, allow_2d\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\py310\\lib\\site-packages\\pandas\\core\\construction.py:602\u001b[0m, in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[0;32m    599\u001b[0m     subarr \u001b[39m=\u001b[39m _try_cast(data, dtype, copy)\n\u001b[0;32m    601\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 602\u001b[0m     subarr \u001b[39m=\u001b[39m maybe_convert_platform(data)\n\u001b[0;32m    603\u001b[0m     \u001b[39mif\u001b[39;00m subarr\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m \u001b[39mobject\u001b[39m:\n\u001b[0;32m    604\u001b[0m         subarr \u001b[39m=\u001b[39m cast(np\u001b[39m.\u001b[39mndarray, subarr)\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\py310\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py:130\u001b[0m, in \u001b[0;36mmaybe_convert_platform\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m    127\u001b[0m arr: ArrayLike\n\u001b[0;32m    129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(values, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m, \u001b[39mrange\u001b[39m)):\n\u001b[1;32m--> 130\u001b[0m     arr \u001b[39m=\u001b[39m construct_1d_object_array_from_listlike(values)\n\u001b[0;32m    131\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m     \u001b[39m# The caller is responsible for ensuring that we have np.ndarray\u001b[39;00m\n\u001b[0;32m    133\u001b[0m     \u001b[39m#  or ExtensionArray here.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     arr \u001b[39m=\u001b[39m values\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\py310\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py:1594\u001b[0m, in \u001b[0;36mconstruct_1d_object_array_from_listlike\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m   1591\u001b[0m \u001b[39m# numpy will try to interpret nested lists as further dimensions, hence\u001b[39;00m\n\u001b[0;32m   1592\u001b[0m \u001b[39m# making a 1D array that contains list-likes is a bit tricky:\u001b[39;00m\n\u001b[0;32m   1593\u001b[0m result \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(\u001b[39mlen\u001b[39m(values), dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1594\u001b[0m result[:] \u001b[39m=\u001b[39m values\n\u001b[0;32m   1595\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\_tensor.py:972\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    970\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    971\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 972\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "df = metadata.copy()\n",
    "df['encoded_audio'] = encoded_audio\n",
    "df['audio_codes'] = audio_codes\n",
    "df['audio_values'] = audio_values\n",
    "df[\"Song Title\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sjufa\\AppData\\Local\\Temp\\ipykernel_9832\\3419225979.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Song Title'][4] = \"Crazy Beautiful\"\n",
      "C:\\Users\\sjufa\\AppData\\Local\\Temp\\ipykernel_9832\\3419225979.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Song Title'][5] = \"Fast Car\"\n",
      "C:\\Users\\sjufa\\AppData\\Local\\Temp\\ipykernel_9832\\3419225979.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Song Title'][11] = \"Growin' up and Gettin' Old\"\n",
      "C:\\Users\\sjufa\\AppData\\Local\\Temp\\ipykernel_9832\\3419225979.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Song Title'][15:] = \"Hurricane\"\n"
     ]
    }
   ],
   "source": [
    "df['Song Title'][4] = \"Crazy Beautiful\"\n",
    "df['Song Title'][5] = \"Fast Car\"\n",
    "df['Song Title'][11] = \"Growin' up and Gettin' Old\"\n",
    "df['Song Title'][15:] = \"Hurricane\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"audio_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2, 1125])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['audio_codes'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['audio_values'] = model.decode(encoder_outputs.audio_codes, encoder_outputs.audio_scales, inputs[\"padding_mask\"])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_samples(encoder_outputs): # load the model + processor (for pre-processing the audio)\n",
    "    \"\"\" Decode the audio samples \"\"\"\n",
    "    model = EncodecModel.from_pretrained(\"facebook/encodec_24khz\")\n",
    "    processor = AutoProcessor.from_pretrained(\"facebook/encodec_24khz\")\n",
    "\n",
    "    # Take the encoder outputs and decode them\n",
    "    audio_values = model.decode(encoder_outputs.audio_codes, encoder_outputs.audio_scales, inputs[\"padding_mask\"])[0]\n",
    "\n",
    "    return audio_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m decoded_audio \u001b[39m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m encoder_outputs \u001b[39min\u001b[39;00m encoded_audio:\n\u001b[1;32m----> 5\u001b[0m     decoded_audio\u001b[39m.\u001b[39mappend(decode_samples(encoder_outputs))\n",
      "Cell \u001b[1;32mIn[19], line 7\u001b[0m, in \u001b[0;36mdecode_samples\u001b[1;34m(encoder_outputs)\u001b[0m\n\u001b[0;32m      4\u001b[0m processor \u001b[39m=\u001b[39m AutoProcessor\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mfacebook/encodec_24khz\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[39m# Take the encoder outputs and decode them\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m audio_values \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mdecode(encoder_outputs\u001b[39m.\u001b[39maudio_codes, encoder_outputs\u001b[39m.\u001b[39maudio_scales, inputs[\u001b[39m\"\u001b[39m\u001b[39mpadding_mask\u001b[39m\u001b[39m\"\u001b[39m])[\u001b[39m0\u001b[39m]\n\u001b[0;32m      9\u001b[0m \u001b[39mreturn\u001b[39;00m audio_values\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "# Iterate through the audio data and decode each segment\n",
    "i = 0\n",
    "decoded_audio = []\n",
    "for encoder_outputs in encoded_audio:\n",
    "    decoded_audio.append(decode_samples(encoder_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'audio_codes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Add the encoded audio and audio codes to the metadata dataframe\u001b[39;00m\n\u001b[0;32m      2\u001b[0m audio_metadata[\u001b[39m'\u001b[39m\u001b[39mencoded_audio\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m encoded_audio\n\u001b[1;32m----> 3\u001b[0m audio_metadata[\u001b[39m'\u001b[39m\u001b[39maudio_codes\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m encoded_audio\u001b[39m.\u001b[39;49maudio_codes\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'audio_codes'"
     ]
    }
   ],
   "source": [
    "# Add the encoded audio and audio codes to the metadata dataframe\n",
    "audio_metadata['encoded_audio'] = encoded_audio\n",
    "audio_metadata['audio_codes'] = encoded_audio.audio_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in the song titles to the metadata dataframe\n",
    "audio_metadata['song_title'] = audio_metadata['file'].apply(lambda x: x.split('/')[-1].split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop everything in the song title up to the first underscore after \"lc\"\n",
    "audio_metadata['song_title'] = audio_metadata['song_title'].apply(lambda x: x.split('_')[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>segment_id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>encoded_audio</th>\n",
       "      <th>audio_codes</th>\n",
       "      <th>song_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./audio_files/artist_main\\lc_beautiful_crazy.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>{'audio_codes': [[tensor([[ 62,  62,  62,  ......</td>\n",
       "      <td>[[[tensor([ 62,  62,  62,  ..., 879, 133, 855]...</td>\n",
       "      <td>artist_main\\lc_beautiful_crazy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./audio_files/artist_main\\lc_beautiful_crazy.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "      <td>{'audio_codes': [[tensor([[855, 904, 999,  ......</td>\n",
       "      <td>[[[tensor([855, 904, 999,  ..., 923, 489, 534]...</td>\n",
       "      <td>artist_main\\lc_beautiful_crazy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./audio_files/artist_main\\lc_beautiful_crazy.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>45</td>\n",
       "      <td>{'audio_codes': [[tensor([[124, 698, 534,  ......</td>\n",
       "      <td>[[[tensor([124, 698, 534,  ..., 967, 393, 561]...</td>\n",
       "      <td>artist_main\\lc_beautiful_crazy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./audio_files/artist_main\\lc_beautiful_crazy.wav</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>60</td>\n",
       "      <td>{'audio_codes': [[tensor([[1010,  751,  393,  ...</td>\n",
       "      <td>[[[tensor([1010,  751,  393,  ...,  828,  167,...</td>\n",
       "      <td>artist_main\\lc_beautiful_crazy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./audio_files/artist_main\\lc_beautiful_crazy.wav</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>75</td>\n",
       "      <td>{'audio_codes': [[tensor([[ 720,  778,  734,  ...</td>\n",
       "      <td>[[[tensor([ 720,  778,  734,  ..., 1006,  462,...</td>\n",
       "      <td>artist_main\\lc_beautiful_crazy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>./audio_files/artist_main\\lc_fastcar.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>{'audio_codes': [[tensor([[ 62,  62,  62,  ......</td>\n",
       "      <td>[[[tensor([ 62,  62,  62,  ..., 219,  70, 363]...</td>\n",
       "      <td>artist_main\\lc_fastcar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>./audio_files/artist_main\\lc_fastcar.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "      <td>{'audio_codes': [[tensor([[804, 949, 645,  ......</td>\n",
       "      <td>[[[tensor([804, 949, 645,  ..., 977, 958, 495]...</td>\n",
       "      <td>artist_main\\lc_fastcar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>./audio_files/artist_main\\lc_fastcar.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>45</td>\n",
       "      <td>{'audio_codes': [[tensor([[958, 916, 901,  ......</td>\n",
       "      <td>[[[tensor([958, 916, 901,  ..., 923, 239, 974]...</td>\n",
       "      <td>artist_main\\lc_fastcar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>./audio_files/artist_main\\lc_fastcar.wav</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>60</td>\n",
       "      <td>{'audio_codes': [[tensor([[681, 815, 182,  ......</td>\n",
       "      <td>[[[tensor([681, 815, 182,  ..., 193, 740, 921]...</td>\n",
       "      <td>artist_main\\lc_fastcar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>./audio_files/artist_main\\lc_fastcar.wav</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>75</td>\n",
       "      <td>{'audio_codes': [[tensor([[ 969, 1008,  761,  ...</td>\n",
       "      <td>[[[tensor([ 969, 1008,  761,  ...,  342,  689,...</td>\n",
       "      <td>artist_main\\lc_fastcar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>./audio_files/artist_main\\lc_growinup_gettinol...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>{'audio_codes': [[tensor([[ 62,  62,  62,  ......</td>\n",
       "      <td>[[[tensor([ 62,  62,  62,  ..., 936, 955, 921]...</td>\n",
       "      <td>artist_main\\lc_growinup_gettinold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>./audio_files/artist_main\\lc_growinup_gettinol...</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "      <td>{'audio_codes': [[tensor([[ 921,  942, 1004,  ...</td>\n",
       "      <td>[[[tensor([ 921,  942, 1004,  ...,   18,  884,...</td>\n",
       "      <td>artist_main\\lc_growinup_gettinold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>./audio_files/artist_main\\lc_growinup_gettinol...</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>45</td>\n",
       "      <td>{'audio_codes': [[tensor([[120, 914, 584,  ......</td>\n",
       "      <td>[[[tensor([120, 914, 584,  ..., 345, 345,  33]...</td>\n",
       "      <td>artist_main\\lc_growinup_gettinold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>./audio_files/artist_main\\lc_growinup_gettinol...</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>60</td>\n",
       "      <td>{'audio_codes': [[tensor([[345,  33, 901,  ......</td>\n",
       "      <td>[[[tensor([345,  33, 901,  ..., 970, 654, 715]...</td>\n",
       "      <td>artist_main\\lc_growinup_gettinold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>./audio_files/artist_main\\lc_growinup_gettinol...</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>75</td>\n",
       "      <td>{'audio_codes': [[tensor([[202,  34, 527,  ......</td>\n",
       "      <td>[[[tensor([202,  34, 527,  ..., 485, 652, 601]...</td>\n",
       "      <td>artist_main\\lc_growinup_gettinold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>./audio_files/artist_main\\lc_hurricane.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>{'audio_codes': [[tensor([[ 62,  62,  62,  ......</td>\n",
       "      <td>[[[tensor([ 62,  62,  62,  ..., 582, 814, 613]...</td>\n",
       "      <td>artist_main\\lc_hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>./audio_files/artist_main\\lc_hurricane.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "      <td>{'audio_codes': [[tensor([[167, 613, 613,  ......</td>\n",
       "      <td>[[[tensor([167, 613, 613,  ..., 976, 953, 293]...</td>\n",
       "      <td>artist_main\\lc_hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>./audio_files/artist_main\\lc_hurricane.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>45</td>\n",
       "      <td>{'audio_codes': [[tensor([[862, 263, 926,  ......</td>\n",
       "      <td>[[[tensor([862, 263, 926,  ..., 141,  85, 908]...</td>\n",
       "      <td>artist_main\\lc_hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>./audio_files/artist_main\\lc_hurricane.wav</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>60</td>\n",
       "      <td>{'audio_codes': [[tensor([[750, 800, 801,  ......</td>\n",
       "      <td>[[[tensor([750, 800, 801,  ..., 894, 776, 776]...</td>\n",
       "      <td>artist_main\\lc_hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>./audio_files/artist_main\\lc_hurricane.wav</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>75</td>\n",
       "      <td>{'audio_codes': [[tensor([[671, 431, 431,  ......</td>\n",
       "      <td>[[[tensor([671, 431, 431,  ...,  57, 994, 914]...</td>\n",
       "      <td>artist_main\\lc_hurricane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 file  segment_id  start_time  \\\n",
       "0    ./audio_files/artist_main\\lc_beautiful_crazy.wav           0           0   \n",
       "1    ./audio_files/artist_main\\lc_beautiful_crazy.wav           1          15   \n",
       "2    ./audio_files/artist_main\\lc_beautiful_crazy.wav           2          30   \n",
       "3    ./audio_files/artist_main\\lc_beautiful_crazy.wav           3          45   \n",
       "4    ./audio_files/artist_main\\lc_beautiful_crazy.wav           4          60   \n",
       "5            ./audio_files/artist_main\\lc_fastcar.wav           0           0   \n",
       "6            ./audio_files/artist_main\\lc_fastcar.wav           1          15   \n",
       "7            ./audio_files/artist_main\\lc_fastcar.wav           2          30   \n",
       "8            ./audio_files/artist_main\\lc_fastcar.wav           3          45   \n",
       "9            ./audio_files/artist_main\\lc_fastcar.wav           4          60   \n",
       "10  ./audio_files/artist_main\\lc_growinup_gettinol...           0           0   \n",
       "11  ./audio_files/artist_main\\lc_growinup_gettinol...           1          15   \n",
       "12  ./audio_files/artist_main\\lc_growinup_gettinol...           2          30   \n",
       "13  ./audio_files/artist_main\\lc_growinup_gettinol...           3          45   \n",
       "14  ./audio_files/artist_main\\lc_growinup_gettinol...           4          60   \n",
       "15         ./audio_files/artist_main\\lc_hurricane.wav           0           0   \n",
       "16         ./audio_files/artist_main\\lc_hurricane.wav           1          15   \n",
       "17         ./audio_files/artist_main\\lc_hurricane.wav           2          30   \n",
       "18         ./audio_files/artist_main\\lc_hurricane.wav           3          45   \n",
       "19         ./audio_files/artist_main\\lc_hurricane.wav           4          60   \n",
       "\n",
       "    end_time                                      encoded_audio  \\\n",
       "0         15  {'audio_codes': [[tensor([[ 62,  62,  62,  ......   \n",
       "1         30  {'audio_codes': [[tensor([[855, 904, 999,  ......   \n",
       "2         45  {'audio_codes': [[tensor([[124, 698, 534,  ......   \n",
       "3         60  {'audio_codes': [[tensor([[1010,  751,  393,  ...   \n",
       "4         75  {'audio_codes': [[tensor([[ 720,  778,  734,  ...   \n",
       "5         15  {'audio_codes': [[tensor([[ 62,  62,  62,  ......   \n",
       "6         30  {'audio_codes': [[tensor([[804, 949, 645,  ......   \n",
       "7         45  {'audio_codes': [[tensor([[958, 916, 901,  ......   \n",
       "8         60  {'audio_codes': [[tensor([[681, 815, 182,  ......   \n",
       "9         75  {'audio_codes': [[tensor([[ 969, 1008,  761,  ...   \n",
       "10        15  {'audio_codes': [[tensor([[ 62,  62,  62,  ......   \n",
       "11        30  {'audio_codes': [[tensor([[ 921,  942, 1004,  ...   \n",
       "12        45  {'audio_codes': [[tensor([[120, 914, 584,  ......   \n",
       "13        60  {'audio_codes': [[tensor([[345,  33, 901,  ......   \n",
       "14        75  {'audio_codes': [[tensor([[202,  34, 527,  ......   \n",
       "15        15  {'audio_codes': [[tensor([[ 62,  62,  62,  ......   \n",
       "16        30  {'audio_codes': [[tensor([[167, 613, 613,  ......   \n",
       "17        45  {'audio_codes': [[tensor([[862, 263, 926,  ......   \n",
       "18        60  {'audio_codes': [[tensor([[750, 800, 801,  ......   \n",
       "19        75  {'audio_codes': [[tensor([[671, 431, 431,  ......   \n",
       "\n",
       "                                          audio_codes  \\\n",
       "0   [[[tensor([ 62,  62,  62,  ..., 879, 133, 855]...   \n",
       "1   [[[tensor([855, 904, 999,  ..., 923, 489, 534]...   \n",
       "2   [[[tensor([124, 698, 534,  ..., 967, 393, 561]...   \n",
       "3   [[[tensor([1010,  751,  393,  ...,  828,  167,...   \n",
       "4   [[[tensor([ 720,  778,  734,  ..., 1006,  462,...   \n",
       "5   [[[tensor([ 62,  62,  62,  ..., 219,  70, 363]...   \n",
       "6   [[[tensor([804, 949, 645,  ..., 977, 958, 495]...   \n",
       "7   [[[tensor([958, 916, 901,  ..., 923, 239, 974]...   \n",
       "8   [[[tensor([681, 815, 182,  ..., 193, 740, 921]...   \n",
       "9   [[[tensor([ 969, 1008,  761,  ...,  342,  689,...   \n",
       "10  [[[tensor([ 62,  62,  62,  ..., 936, 955, 921]...   \n",
       "11  [[[tensor([ 921,  942, 1004,  ...,   18,  884,...   \n",
       "12  [[[tensor([120, 914, 584,  ..., 345, 345,  33]...   \n",
       "13  [[[tensor([345,  33, 901,  ..., 970, 654, 715]...   \n",
       "14  [[[tensor([202,  34, 527,  ..., 485, 652, 601]...   \n",
       "15  [[[tensor([ 62,  62,  62,  ..., 582, 814, 613]...   \n",
       "16  [[[tensor([167, 613, 613,  ..., 976, 953, 293]...   \n",
       "17  [[[tensor([862, 263, 926,  ..., 141,  85, 908]...   \n",
       "18  [[[tensor([750, 800, 801,  ..., 894, 776, 776]...   \n",
       "19  [[[tensor([671, 431, 431,  ...,  57, 994, 914]...   \n",
       "\n",
       "                           song_title  \n",
       "0      artist_main\\lc_beautiful_crazy  \n",
       "1      artist_main\\lc_beautiful_crazy  \n",
       "2      artist_main\\lc_beautiful_crazy  \n",
       "3      artist_main\\lc_beautiful_crazy  \n",
       "4      artist_main\\lc_beautiful_crazy  \n",
       "5              artist_main\\lc_fastcar  \n",
       "6              artist_main\\lc_fastcar  \n",
       "7              artist_main\\lc_fastcar  \n",
       "8              artist_main\\lc_fastcar  \n",
       "9              artist_main\\lc_fastcar  \n",
       "10  artist_main\\lc_growinup_gettinold  \n",
       "11  artist_main\\lc_growinup_gettinold  \n",
       "12  artist_main\\lc_growinup_gettinold  \n",
       "13  artist_main\\lc_growinup_gettinold  \n",
       "14  artist_main\\lc_growinup_gettinold  \n",
       "15           artist_main\\lc_hurricane  \n",
       "16           artist_main\\lc_hurricane  \n",
       "17           artist_main\\lc_hurricane  \n",
       "18           artist_main\\lc_hurricane  \n",
       "19           artist_main\\lc_hurricane  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoded_audio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m encoded_audio\n",
      "\u001b[1;31mNameError\u001b[0m: name 'encoded_audio' is not defined"
     ]
    }
   ],
   "source": [
    "encoded_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
