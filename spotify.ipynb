{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the lyrics.json file\n",
    "with open('./utils/lyrics.json') as json_file:\n",
    "    lyrics = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['lyrics', 'title', 'artist', 'album', 'source', 'music', 'description'])\n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 elements\n",
    "print(lyrics['Crash Into Me'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Crash Into Me', 'Ants Marching', '#41', 'Two Step', 'Grey Street', 'Warehouse', 'Samurai Cop (Oh Joy Begin)', 'Satellite', 'The Space Between', 'Tripping Billies', 'Jimi Thing', 'Crush', 'You and Me', 'Granny', 'Say Goodbye', 'The Stone', 'Grace Is Gone', 'Christmas Song', 'Dancing Nancies', 'Bartender', 'Lie in Our Graves', 'Cry Freedom', 'What Would You Say', 'Come On Come On', 'The Song That Jane Likes'])\n"
     ]
    }
   ],
   "source": [
    "print(lyrics.keys())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load song metadata from JSON\n",
    "with open('./utils/lyrics.json', 'r') as f:\n",
    "    song_metadata = json.load(f)\n",
    "\n",
    "song_name_list = ['Crash Into Me', 'Ants Marching', '#41', 'Two Step',\n",
    "                'Grey Street', 'Warehouse', 'Samurai Cop (Oh Joy Begin)',\n",
    "                'Satellite', 'The Space Between', 'Tripping Billies',\n",
    "                'Jimi Thing', 'Crush', 'You and Me']\n",
    "\n",
    "# If the keys are not in the song_name_list, remove them\n",
    "for key in list(song_metadata.keys()):\n",
    "    if key not in song_name_list:\n",
    "        del song_metadata[key]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be55272b631479e8f014d13402b8e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/265 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import librosa\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists('./data/dmb_audio'):\n",
    "    os.makedirs('./data/dmb_audio')\n",
    "\n",
    "# Function to load and segment audio\n",
    "def load_and_segment_audio(file_path, song_title, segment_length=15):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    audio_length = librosa.get_duration(y=y, sr=sr)\n",
    "    num_segments = int(audio_length // segment_length)\n",
    "    remainder = audio_length % segment_length\n",
    "    segments = []\n",
    "    metadata = []\n",
    "\n",
    "    for i in range(num_segments):\n",
    "        start = int(i * segment_length * sr)\n",
    "        end = int((i + 1) * segment_length * sr)\n",
    "        segments.append(y[start:end])\n",
    "        metadata.append({'segment_num': i, 'is_padded': False})\n",
    "    \n",
    "    if remainder > 0:\n",
    "        start = int(num_segments * segment_length * sr)\n",
    "        end = int((num_segments + 1) * segment_length * sr)\n",
    "        padded_segment = np.pad(y[start:], (0, end - start - len(y[start:])))\n",
    "        segments.append(padded_segment)\n",
    "        metadata.append({'segment_num': num_segments, 'is_padded': True})\n",
    "\n",
    "    for i, segment in enumerate(segments):\n",
    "        new_file_path = f\"./data/dmb_audio/{song_title.replace(' ', '_')}_{i}.wav\"\n",
    "        write(new_file_path, sr, (segment * 32767).astype(np.int16))  # Convert float to 16-bit PCM format\n",
    "        metadata[i]['file_name'] = new_file_path\n",
    "\n",
    "    return segments, metadata\n",
    "\n",
    "# Initialize empty lists to hold all segments and metadata across multiple songs\n",
    "all_segments = []\n",
    "# Initialize empty list to hold metadata for all segments across multiple songs\n",
    "all_metadata = []\n",
    "\n",
    "# Iterate over each song's metadata and corresponding audio file\n",
    "for song_title, metadata in song_metadata.items():\n",
    "    if song_title != 'Crash Into Me':\n",
    "        audio_file = f\"../dmb_audio/{song_title}.wav\"  # Replace with the actual file path\n",
    "        segments, segment_metadata = load_and_segment_audio(audio_file, song_title)\n",
    "\n",
    "        for i, seg_meta in enumerate(segment_metadata):\n",
    "            # Combine song-specific metadata with segment-specific metadata\n",
    "            combined_metadata = {\n",
    "                'lyrics': metadata.get('lyrics', None),\n",
    "                'title': metadata.get('title', None),\n",
    "                'artist': metadata.get('artist', None),\n",
    "                'album': metadata.get('album', None),\n",
    "                'source': metadata.get('source', None),\n",
    "                'description': metadata.get('description', None),\n",
    "                'segment_num': seg_meta['segment_num'],\n",
    "                'is_padded': seg_meta['is_padded'],\n",
    "                'file_name': seg_meta['file_name']\n",
    "                #'music': segments[i]  # Assuming you want to include the segment itself\n",
    "            }\n",
    "            all_metadata.append(combined_metadata)\n",
    "    else:\n",
    "        pass\n",
    "# Create Hugging Face Dataset\n",
    "dataset = Dataset.from_dict({\n",
    "    'metadata': all_metadata\n",
    "})\n",
    "\n",
    "# Save the dataset\n",
    "dataset.save_to_disk('./utils/dataset/')  # Replace with your desired path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a .jsonl file for the metadata\n",
    "import json\n",
    "for data in dataset[\"metadata\"]:\n",
    "    with open('./utils/dataset/metadata2.jsonl', 'a') as f:\n",
    "        json.dump(data, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't cast\nmetadata: struct<album: string, artist: string, description: string, file_name: string, is_padded: bool, lyrics: string, segment_num: int64, source: string, title: string>\n  child 0, album: string\n  child 1, artist: string\n  child 2, description: string\n  child 3, file_name: string\n  child 4, is_padded: bool\n  child 5, lyrics: string\n  child 6, segment_num: int64\n  child 7, source: string\n  child 8, title: string\n-- schema metadata --\nhuggingface: '{\"info\": {\"features\": {\"metadata\": {\"album\": {\"dtype\": \"str' + 431\nto\n{'metadata': {'album': Value(dtype='string', id=None), 'artist': Value(dtype='string', id=None), 'description': Value(dtype='string', id=None), 'file_name': Value(dtype='string', id=None), 'is_padded': Value(dtype='bool', id=None), 'lyrics': Value(dtype='string', id=None), 'segment_num': Value(dtype='int64', id=None), 'source': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None)}, 'file_name': Audio(sampling_rate=None, mono=True, decode=True, id=None)}\nbecause column names don't match",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sjufa\\OneDrive\\Desktop\\Current Projects\\vl_demo\\spotify.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sjufa/OneDrive/Desktop/Current%20Projects/vl_demo/spotify.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m Audio\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sjufa/OneDrive/Desktop/Current%20Projects/vl_demo/spotify.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m audio_dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mcast_column(\u001b[39m\"\u001b[39;49m\u001b[39mfile_name\u001b[39;49m\u001b[39m\"\u001b[39;49m, Audio())\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\vclone1\\lib\\site-packages\\datasets\\fingerprint.py:511\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    507\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[0;32m    509\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[1;32m--> 511\u001b[0m out \u001b[39m=\u001b[39m func(dataset, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    513\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\vclone1\\lib\\site-packages\\datasets\\arrow_dataset.py:2104\u001b[0m, in \u001b[0;36mDataset.cast_column\u001b[1;34m(self, column, feature, new_fingerprint)\u001b[0m\n\u001b[0;32m   2102\u001b[0m dataset\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mfeatures[column] \u001b[39m=\u001b[39m feature\n\u001b[0;32m   2103\u001b[0m dataset\u001b[39m.\u001b[39m_fingerprint \u001b[39m=\u001b[39m new_fingerprint\n\u001b[1;32m-> 2104\u001b[0m dataset\u001b[39m.\u001b[39m_data \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49m_data\u001b[39m.\u001b[39;49mcast(dataset\u001b[39m.\u001b[39;49mfeatures\u001b[39m.\u001b[39;49marrow_schema)\n\u001b[0;32m   2105\u001b[0m dataset\u001b[39m.\u001b[39m_data \u001b[39m=\u001b[39m update_metadata_with_features(dataset\u001b[39m.\u001b[39m_data, dataset\u001b[39m.\u001b[39mfeatures)\n\u001b[0;32m   2106\u001b[0m \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\vclone1\\lib\\site-packages\\datasets\\table.py:901\u001b[0m, in \u001b[0;36mInMemoryTable.cast\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcast\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    889\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \u001b[39m    Cast table values to another schema.\u001b[39;00m\n\u001b[0;32m    891\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[39m        `datasets.table.Table`\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mreturn\u001b[39;00m InMemoryTable(table_cast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtable, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\vclone1\\lib\\site-packages\\datasets\\table.py:2328\u001b[0m, in \u001b[0;36mtable_cast\u001b[1;34m(table, schema)\u001b[0m\n\u001b[0;32m   2314\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Improved version of `pa.Table.cast`.\u001b[39;00m\n\u001b[0;32m   2315\u001b[0m \n\u001b[0;32m   2316\u001b[0m \u001b[39mIt supports casting to feature types stored in the schema metadata.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2325\u001b[0m \u001b[39m    table (`pyarrow.Table`): the casted table\u001b[39;00m\n\u001b[0;32m   2326\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2327\u001b[0m \u001b[39mif\u001b[39;00m table\u001b[39m.\u001b[39mschema \u001b[39m!=\u001b[39m schema:\n\u001b[1;32m-> 2328\u001b[0m     \u001b[39mreturn\u001b[39;00m cast_table_to_schema(table, schema)\n\u001b[0;32m   2329\u001b[0m \u001b[39melif\u001b[39;00m table\u001b[39m.\u001b[39mschema\u001b[39m.\u001b[39mmetadata \u001b[39m!=\u001b[39m schema\u001b[39m.\u001b[39mmetadata:\n\u001b[0;32m   2330\u001b[0m     \u001b[39mreturn\u001b[39;00m table\u001b[39m.\u001b[39mreplace_schema_metadata(schema\u001b[39m.\u001b[39mmetadata)\n",
      "File \u001b[1;32mc:\\Users\\sjufa\\anaconda3\\envs\\vclone1\\lib\\site-packages\\datasets\\table.py:2286\u001b[0m, in \u001b[0;36mcast_table_to_schema\u001b[1;34m(table, schema)\u001b[0m\n\u001b[0;32m   2284\u001b[0m features \u001b[39m=\u001b[39m Features\u001b[39m.\u001b[39mfrom_arrow_schema(schema)\n\u001b[0;32m   2285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39msorted\u001b[39m(table\u001b[39m.\u001b[39mcolumn_names) \u001b[39m!=\u001b[39m \u001b[39msorted\u001b[39m(features):\n\u001b[1;32m-> 2286\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt cast\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mtable\u001b[39m.\u001b[39mschema\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mto\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfeatures\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mbecause column names don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2287\u001b[0m arrays \u001b[39m=\u001b[39m [cast_array_to_feature(table[name], feature) \u001b[39mfor\u001b[39;00m name, feature \u001b[39min\u001b[39;00m features\u001b[39m.\u001b[39mitems()]\n\u001b[0;32m   2288\u001b[0m \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mTable\u001b[39m.\u001b[39mfrom_arrays(arrays, schema\u001b[39m=\u001b[39mschema)\n",
      "\u001b[1;31mValueError\u001b[0m: Couldn't cast\nmetadata: struct<album: string, artist: string, description: string, file_name: string, is_padded: bool, lyrics: string, segment_num: int64, source: string, title: string>\n  child 0, album: string\n  child 1, artist: string\n  child 2, description: string\n  child 3, file_name: string\n  child 4, is_padded: bool\n  child 5, lyrics: string\n  child 6, segment_num: int64\n  child 7, source: string\n  child 8, title: string\n-- schema metadata --\nhuggingface: '{\"info\": {\"features\": {\"metadata\": {\"album\": {\"dtype\": \"str' + 431\nto\n{'metadata': {'album': Value(dtype='string', id=None), 'artist': Value(dtype='string', id=None), 'description': Value(dtype='string', id=None), 'file_name': Value(dtype='string', id=None), 'is_padded': Value(dtype='bool', id=None), 'lyrics': Value(dtype='string', id=None), 'segment_num': Value(dtype='int64', id=None), 'source': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None)}, 'file_name': Audio(sampling_rate=None, mono=True, decode=True, id=None)}\nbecause column names don't match"
     ]
    }
   ],
   "source": [
    "from datasets import Audio\n",
    "audio_dataset = dataset.cast_column(\"file_name\", Audio())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vclone1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
